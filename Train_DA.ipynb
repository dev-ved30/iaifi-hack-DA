{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866b5eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path    \n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34cd0f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from C:\\Users\\qwsaz\\Projects\\iaifi_su25\\iaifi-hack-DA\\data\\BTS\\train.parquet\n",
      "\n",
      "Before transforms and mappings, the dataset contains...\n",
      "          Class  Counts\n",
      "0           AGN     284\n",
      "1          AGN?      13\n",
      "2        AM CVn       3\n",
      "3        BL Lac       1\n",
      "4        Blazar       2\n",
      "5         CLAGN       3\n",
      "6            CV     590\n",
      "7           CV?     118\n",
      "8       Ca-rich       6\n",
      "9          FBOT       3\n",
      "10         ILRT       4\n",
      "11          Ien       1\n",
      "12          LBV       3\n",
      "13        LINER       1\n",
      "14          LRN       3\n",
      "15         NLS1       3\n",
      "16       NLSy1?       2\n",
      "17          QSO      59\n",
      "18       SLSN-I      75\n",
      "19     SLSN-I.5       3\n",
      "20      SLSN-I?       2\n",
      "21      SLSN-II      19\n",
      "22        SN II    1106\n",
      "23     SN II-SL       6\n",
      "24   SN II-norm      10\n",
      "25    SN II-pec       2\n",
      "26       SN II?      19\n",
      "27       SN IIL       1\n",
      "28       SN IIP      74\n",
      "29       SN IIb     109\n",
      "30   SN IIb-pec       1\n",
      "31      SN IIb?       4\n",
      "32       SN IIn     194\n",
      "33      SN IIn?       4\n",
      "34        SN Ia    5150\n",
      "35   SN Ia-00cx       3\n",
      "36   SN Ia-03fg      17\n",
      "37    SN Ia-91T     149\n",
      "38   SN Ia-91bg      61\n",
      "39  SN Ia-91bg?       2\n",
      "40    SN Ia-CSM      19\n",
      "41   SN Ia-CSM?       2\n",
      "42   SN Ia-norm      53\n",
      "43    SN Ia-pec      31\n",
      "44       SN Ia?      27\n",
      "45       SN Iax      30\n",
      "46        SN Ib     128\n",
      "47    SN Ib-pec       2\n",
      "48      SN Ib/c      19\n",
      "49     SN Ib/c?       2\n",
      "50       SN Ib?       3\n",
      "51       SN Ibn      27\n",
      "52      SN Ibn?       1\n",
      "53        SN Ic     152\n",
      "54     SN Ic-BL      55\n",
      "55    SN Ic-BL?       2\n",
      "56     SN Ic-SL       1\n",
      "57       SN Ic?       7\n",
      "58       SN Icn       2\n",
      "59      Seyfert       3\n",
      "60          TDE      57\n",
      "61       blazar       3\n",
      "62       bogus?       3\n",
      "63         nova      32\n",
      "64    nova-like       3\n",
      "65        nova?       1\n",
      "66         rock       1\n",
      "67         star       3\n",
      "68      varstar       2\n",
      "Starting Dataset Transformations:\n",
      "Subtracting time of first observation...\n",
      "Replacing band labels with mean wavelengths...\n",
      "Mapping BTS samples explorer classes to astrophysical classes...\n",
      "Done!\n",
      "\n",
      "Excluding ['Anomaly'] from the dataset...\n",
      "Loading dataset from C:\\Users\\qwsaz\\Projects\\iaifi_su25\\iaifi-hack-DA\\data\\BTS\\val.parquet\n",
      "\n",
      "Before transforms and mappings, the dataset contains...\n",
      "         Class  Counts\n",
      "0          AGN      24\n",
      "1         AGN?       3\n",
      "2        CLAGN       1\n",
      "3           CV      61\n",
      "4          CV?       7\n",
      "5         ILRT       2\n",
      "6       NLSy1?       1\n",
      "7          QSO       5\n",
      "8         QSO?       1\n",
      "9       SLSN-I       3\n",
      "10     SLSN-II       2\n",
      "11       SN II     108\n",
      "12    SN II-SL       3\n",
      "13      SN II?       5\n",
      "14      SN IIP      14\n",
      "15      SN IIb       8\n",
      "16      SN IIn      19\n",
      "17     SN IIn?       1\n",
      "18       SN Ia     563\n",
      "19  SN Ia-03fg       1\n",
      "20   SN Ia-91T      21\n",
      "21  SN Ia-91bg       3\n",
      "22   SN Ia-CSM       1\n",
      "23  SN Ia-norm       4\n",
      "24   SN Ia-pec       1\n",
      "25      SN Ia?       3\n",
      "26      SN Iax       2\n",
      "27       SN Ib      22\n",
      "28   SN Ib-pec       1\n",
      "29     SN Ib/c       3\n",
      "30      SN Ibn       5\n",
      "31       SN Ic      19\n",
      "32    SN Ic-BL       4\n",
      "33      SN Ic?       1\n",
      "34         TDE       6\n",
      "35   afterglow       1\n",
      "36        nova       5\n",
      "Starting Dataset Transformations:\n",
      "Subtracting time of first observation...\n",
      "Replacing band labels with mean wavelengths...\n",
      "Mapping BTS samples explorer classes to astrophysical classes...\n",
      "Done!\n",
      "\n",
      "Excluding ['Anomaly'] from the dataset...\n",
      "Loading dataset from C:\\Users\\qwsaz\\Projects\\iaifi_su25\\iaifi-hack-DA\\data\\ZTF_sims\\train.parquet\n",
      "\n",
      "Before transforms and mappings, the dataset contains...\n",
      "       Class  Counts\n",
      "         AGN    3216\n",
      "        CART    3294\n",
      "         EBE    1624\n",
      "        ILOT     345\n",
      "          KN    1263\n",
      "        PISN    5773\n",
      "      SLSN-I    1622\n",
      "     SNCC-II    2517\n",
      "    SNCC-Ibc    3848\n",
      "   SNIa-91bg    2886\n",
      " SNIa-normal    3754\n",
      "     SNIa-x     2687\n",
      "         TDE    3080\n",
      "uLens-Binary    2213\n",
      "Starting Dataset Transformations:\n",
      "Replacing band labels with mean wavelengths...\n",
      "Dropping saturations from MJD series...\n",
      "Dropping saturations from FLUXCAL series...\n",
      "Dropping saturations from FLUXCALERR series...\n",
      "Dropping saturations from FLT series...\n",
      "Removing saturations from PHOTFLAG series...\n",
      "Replacing PHOTFLAG bitmask with binary values...\n",
      "Subtracting time of first observation...\n",
      "Mapping ZTF sim classes to astrophysical classes...\n",
      "Replacing missing values in MWEBV series...\n",
      "Replacing missing values in MWEBV_ERR series...\n",
      "Replacing missing values in REDSHIFT_HELIO series...\n",
      "Replacing missing values in REDSHIFT_HELIO_ERR series...\n",
      "Done!\n",
      "\n",
      "Excluding [] from the dataset...\n",
      "Loading dataset from C:\\Users\\qwsaz\\Projects\\iaifi_su25\\iaifi-hack-DA\\data\\ZTF_sims\\val.parquet\n",
      "\n",
      "Before transforms and mappings, the dataset contains...\n",
      "       Class  Counts\n",
      "         AGN     342\n",
      "        CART     359\n",
      "         EBE     186\n",
      "        ILOT      30\n",
      "          KN     125\n",
      "        PISN     628\n",
      "      SLSN-I     189\n",
      "     SNCC-II     311\n",
      "    SNCC-Ibc     443\n",
      "   SNIa-91bg     342\n",
      " SNIa-normal     397\n",
      "     SNIa-x      300\n",
      "         TDE     339\n",
      "uLens-Binary     244\n",
      "Starting Dataset Transformations:\n",
      "Replacing band labels with mean wavelengths...\n",
      "Dropping saturations from MJD series...\n",
      "Dropping saturations from FLUXCAL series...\n",
      "Dropping saturations from FLUXCALERR series...\n",
      "Dropping saturations from FLT series...\n",
      "Removing saturations from PHOTFLAG series...\n",
      "Replacing PHOTFLAG bitmask with binary values...\n",
      "Subtracting time of first observation...\n",
      "Mapping ZTF sim classes to astrophysical classes...\n",
      "Replacing missing values in MWEBV series...\n",
      "Replacing missing values in MWEBV_ERR series...\n",
      "Replacing missing values in REDSHIFT_HELIO series...\n",
      "Replacing missing values in REDSHIFT_HELIO_ERR series...\n",
      "Done!\n",
      "\n",
      "Excluding ['Anomaly'] from the dataset...\n"
     ]
    }
   ],
   "source": [
    "from AD.presets import get_train_loader, get_val_loader\n",
    "\n",
    "target_train_loader, _ = get_train_loader('BTS-lite',256,None,excluded_classes=['Anomaly'])\n",
    "target_val_loader, _ = get_val_loader('BTS-lite',256,excluded_classes=['Anomaly'])\n",
    "\n",
    "source_train_loader, _ = get_train_loader('ZTFSims',256,None,excluded_classes=['Anomaly'])\n",
    "source_val_loader, _ = get_val_loader('ZTFSims',256,excluded_classes=['Anomaly'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import geomloss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    epsilon = 1e-6\n",
    "    p = torch.clamp(p, min=epsilon)\n",
    "    q = torch.clamp(q, min=epsilon)\n",
    "    return torch.sum(p * torch.log(p / q), dim=-1)\n",
    "\n",
    "\n",
    "def jensen_shannon_divergence(p, q):\n",
    "    m = 0.5 * (p + q)\n",
    "    jsd = 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "    return jsd\n",
    "\n",
    "\n",
    "def jensen_shannon_distance(p, q):\n",
    "    jsd = jensen_shannon_divergence(p, q)\n",
    "    jsd = torch.clamp(jsd, min=0.0)\n",
    "    return torch.sqrt(jsd)\n",
    "\n",
    "\n",
    "def sinkhorn_loss(\n",
    "    x,\n",
    "    y,\n",
    "    blur,\n",
    "):\n",
    "    loss = geomloss.SamplesLoss(\"sinkhorn\", blur=blur, scaling=0.9, reach=None)\n",
    "    return loss(x, y)\n",
    "\n",
    "\n",
    "def set_all_seeds(num):\n",
    "    random.seed(num)\n",
    "    np.random.seed(num)\n",
    "    torch.manual_seed(num)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(num)\n",
    "\n",
    "\n",
    "def train_SIDDA(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    target_dataloader: DataLoader,\n",
    "    target_val_dataloader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    model_name: str,\n",
    "    scheduler: optim.lr_scheduler = None,\n",
    "    epochs: int = 100,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    save_dir: str = \"checkpoints\",\n",
    "    early_stopping_patience: int = 10,\n",
    "    report_interval: int = 1,\n",
    "):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        model.to(device)\n",
    "\n",
    "    warmup = 0#!!!\n",
    "    print(\"Model Loaded to Device!\")\n",
    "    best_val_acc, best_classification_loss, best_DA_loss, best_total_val_loss = (\n",
    "        0,\n",
    "        float(\"inf\"),\n",
    "        float(\"inf\"),\n",
    "        float(\"inf\"),\n",
    "    )\n",
    "    no_improvement_count = 0\n",
    "    losses, steps = [], []\n",
    "    train_classification_losses, train_DA_losses = [], []\n",
    "    val_losses, val_classification_losses, val_DA_losses = [], [], []\n",
    "    max_distances, epoch_max_distances = [], []\n",
    "    js_distances, epoch_js_distances = [], []\n",
    "    blur_vals, epoch_blur_vals = [], []\n",
    "    eta_1_vals, eta_2_vals = [], []\n",
    "\n",
    "    print(\"Training Started!\")\n",
    "\n",
    "    eta_1 = torch.nn.Parameter(torch.tensor(1.0, device=device))\n",
    "    eta_2 = torch.nn.Parameter(torch.tensor(1.0, device=device))\n",
    "\n",
    "    optimizer.add_param_group({\"params\": [eta_1, eta_2]})\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        classification_losses, DA_losses = [], []\n",
    "\n",
    "        for i, (batch, target_batch) in tqdm(\n",
    "            enumerate(zip(train_dataloader, target_dataloader))\n",
    "        ):\n",
    "            source_inputs = batch\n",
    "            source_outputs = batch['label']\n",
    "            source_outputs = np.array([strname_to_classid[i] for i in source_outputs])\n",
    "            # source_inputs, source_outputs = (\n",
    "            #     source_inputs.to(device).float(),\n",
    "            #     source_outputs.to(device),\n",
    "            # )\n",
    "\n",
    "            target_inputs = target_batch\n",
    "            #target_inputs = target_inputs.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if epoch < warmup:\n",
    "                _, model_outputs = model(source_inputs)\n",
    "                classification_loss = F.cross_entropy(model_outputs, torch.from_numpy(source_outputs))\n",
    "                loss = classification_loss\n",
    "                DA_loss = None\n",
    "            else:\n",
    "\n",
    "                batch_size = len(source_inputs['label'])\n",
    "\n",
    "                source_features, source_model_outputs = model(source_inputs)\n",
    "                target_features, target_model_outputs = model(target_inputs)\n",
    "                \n",
    "\n",
    "                classification_loss = F.cross_entropy(\n",
    "                    source_model_outputs, torch.from_numpy(source_outputs)\n",
    "                )\n",
    "\n",
    "                pairwise_distances = torch.cdist(source_features, target_features, p=2)\n",
    "                flattened_distances = pairwise_distances.view(-1)\n",
    "                max_distance = torch.max(flattened_distances)\n",
    "                max_distances.append(max_distance.detach().cpu().numpy())\n",
    "                js_distances.append(\n",
    "                    jensen_shannon_distance(source_features, target_features)\n",
    "                    .nanmean()\n",
    "                    .item()\n",
    "                )\n",
    "\n",
    "                dynamic_blur_val = 0.05 * max_distance.detach().cpu().numpy()\n",
    "                blur_vals.append(dynamic_blur_val)\n",
    "\n",
    "                DA_loss = sinkhorn_loss(\n",
    "                    source_features,\n",
    "                    target_features,\n",
    "                    blur=max(dynamic_blur_val, 0.01),  # Apply lower bound to blur\n",
    "                )\n",
    "\n",
    "                loss = (\n",
    "                    (1 / (2 * eta_1**2)) * classification_loss\n",
    "                    + (1 / (2 * eta_2**2)) * DA_loss\n",
    "                    + torch.log(torch.abs(eta_1) * torch.abs(eta_2))\n",
    "                )\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            eta_1.data.clamp_(min=1e-3)\n",
    "            eta_2.data.clamp_(min=0.25 * eta_1.data.item())\n",
    "            eta_1_vals.append(eta_1.item())\n",
    "            eta_2_vals.append(eta_2.item())\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            classification_losses.append(classification_loss.item())\n",
    "            \n",
    "            if epoch >= warmup:\n",
    "                DA_losses.append(DA_loss.item())\n",
    "            \n",
    "\n",
    "        mean_max_distance = np.mean(max_distances)\n",
    "        epoch_max_distances.append(mean_max_distance)\n",
    "\n",
    "        mean_blur_val = np.mean(blur_vals)\n",
    "        epoch_blur_vals.append(mean_blur_val)\n",
    "        mean_js_distance = np.nanmean(js_distances)\n",
    "        epoch_js_distances.append(mean_js_distance)\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_classification_loss = np.mean(classification_losses)\n",
    "        train_DA_loss = np.mean(DA_losses) if DA_losses else None\n",
    "\n",
    "        losses.append(train_loss)\n",
    "        train_classification_losses.append(train_classification_loss)\n",
    "        train_DA_losses.append(train_DA_loss)\n",
    "        steps.append(epoch + 1)\n",
    "\n",
    "        if epoch >= warmup:\n",
    "            print(\n",
    "                f\"Epoch: {epoch + 1}, eta_1: {eta_1.item():.4f}, eta_2: {eta_2.item():.4f}\"\n",
    "            )\n",
    "            print(f\"Epoch: {epoch + 1}, Max Distance: {max_distance:.4f}\")\n",
    "\n",
    "        if epoch < warmup:\n",
    "            print(f\"Epoch: {epoch + 1}, Train Loss: {train_loss:.4e}\")\n",
    "            print(\n",
    "                f\"Epoch: {epoch + 1}, Classification Loss: {train_classification_loss:.4e}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Epoch: {epoch + 1}, Train Loss: {train_loss:.4e}\")\n",
    "            print(\n",
    "                f\"Epoch: {epoch + 1}, Classification Loss: {train_classification_loss:.4e}, DA Loss: {train_DA_loss:.4e}\"\n",
    "            )\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % report_interval == 0:\n",
    "            model.eval()\n",
    "            source_correct, source_total, val_loss = (\n",
    "                0,\n",
    "                0,\n",
    "                0.0,\n",
    "            )\n",
    "            val_classification_loss, val_DA_loss = 0.0, 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, (batch, target_batch) in enumerate(\n",
    "                    zip(val_dataloader, target_val_dataloader)\n",
    "                ):\n",
    "                    source_inputs = batch\n",
    "                    source_outputs = batch['label']\n",
    "                    source_outputs = np.array([strname_to_classid[i] for i in source_outputs])\n",
    "                    # source_inputs, source_outputs = (\n",
    "                    #     source_inputs.to(device).float(),\n",
    "                    #     source_outputs.to(device),\n",
    "                    # )\n",
    "\n",
    "                    target_inputs = target_batch\n",
    "                    #target_inputs = target_inputs.to(device).float()\n",
    "\n",
    "\n",
    "\n",
    "                    if epoch < warmup:\n",
    "                        _, source_preds = model(source_inputs)\n",
    "                        classification_loss_ = F.cross_entropy(\n",
    "                            source_preds, torch.from_numpy(source_outputs)\n",
    "                        )\n",
    "                        combined_loss = classification_loss_\n",
    "                        DA_loss_ = 0.0\n",
    "\n",
    "                    else:\n",
    "                        \n",
    "                        source_features, source_preds = model(source_inputs)\n",
    "                        target_features, target_preds = model(target_inputs)  \n",
    "\n",
    "                        classification_loss_ = F.cross_entropy(\n",
    "                            source_preds, torch.from_numpy(source_outputs)\n",
    "                        )\n",
    "\n",
    "                        pairwise_distances = torch.cdist(\n",
    "                            source_features, target_features, p=2\n",
    "                        )\n",
    "                        flattened_distances = pairwise_distances.view(-1)\n",
    "                        max_distance = torch.max(flattened_distances)\n",
    "\n",
    "                        dynamic_blur_val = 0.05 * max_distance.detach().cpu().numpy()\n",
    "                        DA_loss_ = sinkhorn_loss(\n",
    "                            source_features,\n",
    "                            target_features,\n",
    "                            blur=max(dynamic_blur_val, 0.01),\n",
    "                        )\n",
    "\n",
    "                        combined_loss = classification_loss_ + DA_loss_\n",
    "\n",
    "                    val_loss += combined_loss.item()\n",
    "                    val_classification_loss += classification_loss_.item()\n",
    "\n",
    "                    if epoch >= warmup:\n",
    "                        val_DA_loss += DA_loss_.item()\n",
    "\n",
    "                    _, source_predicted = torch.max(source_preds.data, 1)\n",
    "                    source_total += source_outputs.size\n",
    "                    source_correct += (source_predicted == source_outputs).sum().item()\n",
    "\n",
    "            source_val_acc = 100 * source_correct / source_total\n",
    "\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_classification_loss /= len(val_dataloader)\n",
    "\n",
    "            if epoch >= warmup:\n",
    "                val_DA_loss /= len(val_dataloader)\n",
    "\n",
    "            val_losses.append(val_loss)\n",
    "            val_classification_losses.append(val_classification_loss)\n",
    "            val_DA_losses.append(val_DA_loss)\n",
    "\n",
    "            lr = (\n",
    "                scheduler.get_last_lr()[0]\n",
    "                if scheduler is not None\n",
    "                else optimizer.param_groups[0][\"lr\"]\n",
    "            )\n",
    "\n",
    "            if epoch < warmup:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1}, Total Validation Loss: {val_loss:.4f}, Source Validation Accuracy: {source_val_acc:.2f}%, Learning rate: {lr}\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1}, Validation Classification Loss: {val_classification_loss:.4e}\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1}, Total Validation Loss: {val_loss:.4f}, Source Validation Accuracy: {source_val_acc:.2f}%, Learning rate: {lr}\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1}, Validation Classification Loss: {val_classification_loss:.4e}, Validation DA Loss: {val_DA_loss:.4e}\"\n",
    "                )\n",
    "\n",
    "            if val_loss < best_total_val_loss and epoch >= warmup:\n",
    "                best_total_val_loss = val_loss\n",
    "                best_val_epoch = epoch + 1\n",
    "                if torch.cuda.device_count() > 1:\n",
    "                    torch.save(\n",
    "                        model.eval().module.state_dict(),\n",
    "                        os.path.join(save_dir, \"best_model_total_val_loss.pt\"),\n",
    "                    )\n",
    "                else:\n",
    "                    torch.save(\n",
    "                        model.eval().state_dict(),\n",
    "                        os.path.join(save_dir, \"best_model_total_val_loss.pt\"),\n",
    "                    )\n",
    "                print(\n",
    "                    f\"Saved best total validation loss model at epoch {best_val_epoch}\"\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "\n",
    "            if source_val_acc >= best_val_acc:\n",
    "                best_val_acc = source_val_acc\n",
    "                best_val_acc_epoch = epoch + 1\n",
    "                model_path = os.path.join(save_dir, \"best_model_val_acc.pt\")\n",
    "                if torch.cuda.device_count() > 1:\n",
    "                    torch.save(model.eval().module.state_dict(), model_path)\n",
    "                else:\n",
    "                    torch.save(model.eval().state_dict(), model_path)\n",
    "                print(\n",
    "                    f\"Saved best validation accuracy model at epoch {best_val_acc_epoch}\"\n",
    "                )\n",
    "\n",
    "            if val_classification_loss <= best_classification_loss and epoch >= warmup:\n",
    "                best_classification_loss = val_classification_loss\n",
    "                best_classification_loss_epoch = epoch + 1\n",
    "                model_path = os.path.join(save_dir, \"best_model_classification_loss.pt\")\n",
    "                if torch.cuda.device_count() > 1:\n",
    "                    torch.save(model.eval().module.state_dict(), model_path)\n",
    "                else:\n",
    "                    torch.save(model.eval().state_dict(), model_path)\n",
    "                print(\n",
    "                    f\"Saved lowest classification loss model at epoch {best_classification_loss_epoch}\"\n",
    "                )\n",
    "\n",
    "            if val_DA_loss <= best_DA_loss and epoch >= warmup:\n",
    "                best_DA_loss = val_DA_loss\n",
    "                best_DA_epoch = epoch + 1\n",
    "                model_path = os.path.join(save_dir, \"best_model_DA_loss.pt\")\n",
    "                if torch.cuda.device_count() > 1:\n",
    "                    torch.save(model.eval().module.state_dict(), model_path)\n",
    "                else:\n",
    "                    torch.save(model.eval().state_dict(), model_path)\n",
    "                print(f\"Saved lowest DA loss model at epoch {best_DA_epoch}\")\n",
    "\n",
    "            if no_improvement_count >= early_stopping_patience:\n",
    "                print(\n",
    "                    f\"Early stopping after {early_stopping_patience} epochs without improvement in accuracy.\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        torch.save(\n",
    "            model.eval().module.state_dict(), os.path.join(save_dir, \"final_model.pt\")\n",
    "        )\n",
    "    else:\n",
    "        torch.save(model.eval().state_dict(), os.path.join(save_dir, \"final_model.pt\"))\n",
    "\n",
    "    loss_dir = os.path.join(save_dir, \"losses\")\n",
    "    if not os.path.exists(loss_dir):\n",
    "        os.makedirs(loss_dir)\n",
    "\n",
    "    np.save(os.path.join(loss_dir, f\"losses-{model_name}.npy\"), np.array(losses))\n",
    "    np.save(\n",
    "        os.path.join(loss_dir, f\"train_classification_losses-{model_name}.npy\"),\n",
    "        np.array(train_classification_losses),\n",
    "    )\n",
    "    np.save(\n",
    "        os.path.join(loss_dir, f\"train_DA_losses-{model_name}.npy\"),\n",
    "        np.array(train_DA_losses),\n",
    "    )\n",
    "    np.save(\n",
    "        os.path.join(loss_dir, f\"val_losses-{model_name}.npy\"), np.array(val_losses)\n",
    "    )\n",
    "    np.save(\n",
    "        os.path.join(loss_dir, f\"val_classification_losses-{model_name}.npy\"),\n",
    "        np.array(val_classification_losses),\n",
    "    )\n",
    "    np.save(\n",
    "        os.path.join(loss_dir, f\"val_DA_losses-{model_name}.npy\"),\n",
    "        np.array(val_DA_losses),\n",
    "    )\n",
    "    np.save(os.path.join(loss_dir, f\"steps-{model_name}.npy\"), np.array(steps))\n",
    "    np.save(\n",
    "        os.path.join(loss_dir, f\"max_distances-{model_name}.npy\"),\n",
    "        np.array(max_distances),\n",
    "    )\n",
    "    np.save(os.path.join(loss_dir, f\"blur_vals-{model_name}.npy\"), np.array(blur_vals))\n",
    "    np.save(\n",
    "        os.path.join(loss_dir, f\"js_distances-{model_name}.npy\"), np.array(js_distances)\n",
    "    )\n",
    "    np.save(\n",
    "        os.path.join(loss_dir, f\"epoch_max_distances-{model_name}.npy\"),\n",
    "        np.array(epoch_max_distances),\n",
    "    )\n",
    "    np.save(\n",
    "        os.path.join(loss_dir, f\"epoch_blur_vals-{model_name}.npy\"),\n",
    "        np.array(epoch_blur_vals),\n",
    "    )\n",
    "    np.save(\n",
    "        os.path.join(loss_dir, f\"epoch_js_distances-{model_name}.npy\"),\n",
    "        np.array(epoch_js_distances),\n",
    "    )\n",
    "    \n",
    "    np.save(\n",
    "        os.path.join(loss_dir, f\"eta_1_vals-{model_name}.npy\"), \n",
    "        np.array(eta_1_vals)\n",
    "    )\n",
    "    np.save(os.path.join(loss_dir, f\"eta_2_vals-{model_name}.npy\"), \n",
    "            np.array(eta_2_vals)\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Plotting the losses\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    steps = np.array(steps)\n",
    "    validation_steps = steps[::report_interval]\n",
    "    losses = np.array(losses)\n",
    "    train_classification_losses = np.array(train_classification_losses)\n",
    "    train_DA_losses = np.array(train_DA_losses)\n",
    "    val_losses = np.array(val_losses)\n",
    "    val_classification_losses = np.array(val_classification_losses)\n",
    "    val_DA_losses = np.array(val_DA_losses)\n",
    "\n",
    "    # Plot Training Losses\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(steps, losses, label=\"Train Total Loss\")\n",
    "    plt.plot(steps, train_classification_losses, label=\"Train Classification Loss\")\n",
    "    plt.plot(steps, train_DA_losses, label=\"Train DA Loss\")\n",
    "    plt.axvline(x=best_val_epoch, color=\"b\", linestyle=\"--\", label=\"Best Val Epoch\")\n",
    "    plt.axvline(\n",
    "        x=best_classification_loss_epoch,\n",
    "        color=\"y\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"Best Classification Epoch\",\n",
    "    )\n",
    "    plt.axvline(x=best_DA_epoch, color=\"g\", linestyle=\"--\", label=\"Best DA Epoch\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Losses\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Validation Losses\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(validation_steps, val_losses, label=\"Validation Total Loss\")\n",
    "    plt.plot(\n",
    "        validation_steps,\n",
    "        val_classification_losses,\n",
    "        label=\"Validation Classification Loss\",\n",
    "    )\n",
    "    plt.plot(validation_steps, val_DA_losses, label=\"Validation DA Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Validation Losses\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(loss_dir, f\"losses_plot-{model_name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.plot(steps, epoch_max_distances)\n",
    "    plt.axvline(x=best_val_epoch, color=\"b\", linestyle=\"--\", label=\"Best Val Epoch\")\n",
    "    plt.axvline(\n",
    "        x=best_classification_loss_epoch,\n",
    "        color=\"y\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"Best Classification Epoch\",\n",
    "    )\n",
    "    plt.axvline(x=best_DA_epoch, color=\"g\", linestyle=\"--\", label=\"Best DA Epoch\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Max Distance\")\n",
    "    plt.title(\"Max Distance vs. Training Steps\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(loss_dir, f\"max_distance_plot-{model_name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.plot(steps, epoch_blur_vals)\n",
    "    plt.axhline(y=0.01, color=\"r\", linestyle=\"--\")\n",
    "    plt.axhline(y=0.05, color=\"g\", linestyle=\"--\")\n",
    "    plt.axvline(x=best_val_epoch, color=\"b\", linestyle=\"--\", label=\"Best Val Epoch\")\n",
    "    plt.axvline(\n",
    "        x=best_classification_loss_epoch,\n",
    "        color=\"y\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"Best Classification Epoch\",\n",
    "    )\n",
    "    plt.axvline(x=best_DA_epoch, color=\"g\", linestyle=\"--\", label=\"Best DA Epoch\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Blur Value\")\n",
    "    plt.title(\"Blur Value vs. Training Steps\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(loss_dir, f\"blur_value_plot-{model_name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.plot(steps, epoch_js_distances)\n",
    "    plt.axvline(x=best_val_epoch, color=\"b\", linestyle=\"--\", label=\"Best Val Epoch\")\n",
    "    plt.axvline(\n",
    "        x=best_classification_loss_epoch,\n",
    "        color=\"y\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"Best Classification Epoch\",\n",
    "    )\n",
    "    plt.axvline(x=best_DA_epoch, color=\"g\", linestyle=\"--\", label=\"Best DA Epoch\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"JS Distance\")\n",
    "    plt.title(\"JS Distance vs. Training Steps\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(loss_dir, f\"js_distance_plot-{model_name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return (\n",
    "        best_val_epoch,\n",
    "        best_val_acc,\n",
    "        best_classification_loss_epoch,\n",
    "        best_classification_loss,\n",
    "        best_DA_epoch,\n",
    "        best_DA_loss,\n",
    "        losses[-1],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "982b4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for the Hierarchical Classifier\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "    def predict_conditional_probabilities(self, batch):\n",
    "        \n",
    "        logits = self.forward(batch)\n",
    "        conditional_probabilities = F.softmax(logits, dim=-1).detach()\n",
    "        return conditional_probabilities\n",
    "    \n",
    "    def predict_conditional_probabilities_df(self, batch):\n",
    "\n",
    "        level_order_nodes = self.one_hot_encoder.categories_[0]\n",
    "        conditional_probabilities = self.predict_conditional_probabilities(batch)\n",
    "        df = pd.DataFrame(conditional_probabilities, columns=level_order_nodes)\n",
    "        return df\n",
    "    \n",
    "    def get_latent_space_embeddings(self, batch):\n",
    "\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class GRU(Classifier):\n",
    "\n",
    "    def __init__(self, output_dim, ts_feature_dim=5):\n",
    "\n",
    "        super(GRU,  self).__init__()\n",
    "\n",
    "        self.ts_feature_dim = ts_feature_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # recurrent backbone\n",
    "        self.gru = nn.GRU(input_size=ts_feature_dim, hidden_size=100, num_layers=2, batch_first=True)\n",
    "\n",
    "        # post‐GRU dense on time‐series path\n",
    "        self.dense1 = nn.Linear(100, 64)\n",
    "\n",
    "        # merge & head\n",
    "        self.dense2 = nn.Linear(64, 32)\n",
    "\n",
    "        self.dense3 = nn.Linear(32, 16)\n",
    "\n",
    "        self.fc_out = nn.Linear(16, self.output_dim)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def get_latent_space_embeddings(self,batch):\n",
    "\n",
    "        x_ts = batch['ts'] # (batch_size, seq_len, n_ts_features)\n",
    "        lengths = batch['length'] # (batch_size)\n",
    "\n",
    "        # Pack the padded time series data. the lengths vector lets the GRU know the true lengths of each TS, so it can ignore padding\n",
    "        packed = pack_padded_sequence(x_ts, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Recurrent backbone\n",
    "        h0 = torch.zeros(2, x_ts.shape[0], 100).to(x_ts.device)\n",
    "        _, hidden = self.gru(packed, h0)\n",
    "\n",
    "        # Take the last output of the GRU\n",
    "        gru_out = hidden[-1] # (batch_size, hidden_size)\n",
    "\n",
    "        # Post-GRU dense on time-series path\n",
    "        dense1 = self.dense1(gru_out)\n",
    "        dense1 = self.tanh(dense1)\n",
    "\n",
    "        # Merge & head\n",
    "        dense2 = self.dense2(dense1)\n",
    "        dense2 = self.tanh(dense2)\n",
    "\n",
    "        x = self.dense3(dense2)\n",
    "        return x, gru_out\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        # Get the latent space embedding\n",
    "        x_s, gru_out_s = self.get_latent_space_embeddings(batch)\n",
    "        \n",
    "        # Final step to produce logits\n",
    "        x_s = self.relu(x_s)\n",
    "        logits_s = self.fc_out(x_s)\n",
    "\n",
    "        return gru_out_s, logits_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a165ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU(6)\n",
    "opt = torch.optim.AdamW(model.parameters(),1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "236a4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in source_train_loader:\n",
    "    break\n",
    "\n",
    "N = np.unique(X['label'])\n",
    "strname_to_classid = {n:i for i,n in enumerate(N)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d78b3a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded to Device!\n",
      "Training Started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:19, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, eta_1: 1.0010, eta_2: 1.0010\n",
      "Epoch: 1, Max Distance: 6.2295\n",
      "Epoch: 1, Train Loss: 1.4881e-02\n",
      "Epoch: 1, Classification Loss: 1.7960e+00, DA Loss: 2.6087e+00\n",
      "Epoch: 1, Total Validation Loss: 1.4819, Source Validation Accuracy: 0.52%, Learning rate: 0.001\n",
      "Epoch: 1, Validation Classification Loss: 6.0390e-01, Validation DA Loss: 8.7799e-01\n",
      "Saved best total validation loss model at epoch 1\n",
      "Saved best validation accuracy model at epoch 1\n",
      "Saved lowest classification loss model at epoch 1\n",
      "Saved lowest DA loss model at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:57, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, eta_1: 1.0020, eta_2: 1.0020\n",
      "Epoch: 2, Max Distance: 6.5669\n",
      "Epoch: 2, Train Loss: 1.3079e-02\n",
      "Epoch: 2, Classification Loss: 1.7919e+00, DA Loss: 2.0831e+00\n",
      "Epoch: 2, Total Validation Loss: 1.3360, Source Validation Accuracy: 0.00%, Learning rate: 0.001\n",
      "Epoch: 2, Validation Classification Loss: 6.0296e-01, Validation DA Loss: 7.3300e-01\n",
      "Saved best total validation loss model at epoch 2\n",
      "Saved lowest classification loss model at epoch 2\n",
      "Saved lowest DA loss model at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:18, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m training_outputs = train_SIDDA(\n\u001b[32m      2\u001b[39m     model,\n\u001b[32m      3\u001b[39m     source_train_loader,\u001b[38;5;66;03m#source_train_loader,\u001b[39;00m\n\u001b[32m      4\u001b[39m     source_val_loader,\u001b[38;5;66;03m#source_val_loader,\u001b[39;00m\n\u001b[32m      5\u001b[39m     target_train_loader,\n\u001b[32m      6\u001b[39m     target_val_loader,\n\u001b[32m      7\u001b[39m     opt,\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mORACLE_DA\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36mtrain_SIDDA\u001b[39m\u001b[34m(model, train_dataloader, val_dataloader, target_dataloader, target_val_dataloader, optimizer, model_name, scheduler, epochs, device, save_dir, early_stopping_patience, report_interval)\u001b[39m\n\u001b[32m    154\u001b[39m     DA_loss = sinkhorn_loss(\n\u001b[32m    155\u001b[39m         source_features,\n\u001b[32m    156\u001b[39m         target_features,\n\u001b[32m    157\u001b[39m         blur=\u001b[38;5;28mmax\u001b[39m(dynamic_blur_val, \u001b[32m0.01\u001b[39m),  \u001b[38;5;66;03m# Apply lower bound to blur\u001b[39;00m\n\u001b[32m    158\u001b[39m     )\n\u001b[32m    160\u001b[39m     loss = (\n\u001b[32m    161\u001b[39m         (\u001b[32m1\u001b[39m / (\u001b[32m2\u001b[39m * eta_1**\u001b[32m2\u001b[39m)) * classification_loss\n\u001b[32m    162\u001b[39m         + (\u001b[32m1\u001b[39m / (\u001b[32m2\u001b[39m * eta_2**\u001b[32m2\u001b[39m)) * DA_loss\n\u001b[32m    163\u001b[39m         + torch.log(torch.abs(eta_1) * torch.abs(eta_2))\n\u001b[32m    164\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m loss.backward()\n\u001b[32m    167\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m10.0\u001b[39m)\n\u001b[32m    168\u001b[39m eta_1.data.clamp_(\u001b[38;5;28mmin\u001b[39m=\u001b[32m1e-3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qwsaz\\Miniconda3\\envs\\iaifi_DA\\Lib\\site-packages\\torch\\_tensor.py:617\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[32m    574\u001b[39m \n\u001b[32m    575\u001b[39m \u001b[33;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    614\u001b[39m \u001b[33;03m        used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m    620\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    621\u001b[39m         gradient=gradient,\n\u001b[32m    622\u001b[39m         retain_graph=retain_graph,\n\u001b[32m    623\u001b[39m         create_graph=create_graph,\n\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m    626\u001b[39m torch.autograd.backward(\n\u001b[32m    627\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    628\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qwsaz\\Miniconda3\\envs\\iaifi_DA\\Lib\\site-packages\\torch\\overrides.py:1720\u001b[39m, in \u001b[36mhandle_torch_function\u001b[39m\u001b[34m(public_api, relevant_args, *args, **kwargs)\u001b[39m\n\u001b[32m   1716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[32m   1717\u001b[39m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[32m   1718\u001b[39m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[32m   1719\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[32m-> \u001b[39m\u001b[32m1720\u001b[39m         result = mode.__torch_function__(public_api, types, args, kwargs)\n\u001b[32m   1721\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m   1722\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qwsaz\\Miniconda3\\envs\\iaifi_DA\\Lib\\site-packages\\torch\\utils\\_device.py:104\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    103\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qwsaz\\Miniconda3\\envs\\iaifi_DA\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m torch.autograd.backward(\n\u001b[32m    627\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    628\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qwsaz\\Miniconda3\\envs\\iaifi_DA\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m _engine_run_backward(\n\u001b[32m    348\u001b[39m     tensors,\n\u001b[32m    349\u001b[39m     grad_tensors_,\n\u001b[32m    350\u001b[39m     retain_graph,\n\u001b[32m    351\u001b[39m     create_graph,\n\u001b[32m    352\u001b[39m     inputs,\n\u001b[32m    353\u001b[39m     allow_unreachable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    354\u001b[39m     accumulate_grad=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    355\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qwsaz\\Miniconda3\\envs\\iaifi_DA\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable._execution_engine.run_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    824\u001b[39m         t_outputs, *args, **kwargs\n\u001b[32m    825\u001b[39m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "training_outputs = train_SIDDA(\n",
    "    model,\n",
    "    source_train_loader,#source_train_loader,\n",
    "    source_val_loader,#source_val_loader,\n",
    "    target_train_loader,\n",
    "    target_val_loader,\n",
    "    opt,\n",
    "    'ORACLE_DA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iaifi_DA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
